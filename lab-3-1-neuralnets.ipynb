{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UofT FASE ML Bootcamp\n",
    "#### Tuesday August 21, 2019\n",
    "#### Intro to Neural Networks in PyTorch - Lab 1, Day 3 \n",
    "#### Teaching team: Elias Khalil, Kyle E. C. Booth, and Alex Olson \n",
    "##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be taking our first look at developing our own *neural networks* (NN) with [PyTorch](https://pytorch.org/), probably the most popular machine learning library for working with NNs. \n",
    "\n",
    "First, let's get the required packages installed. Run the following command in your command line (or, if you're working with windows, your Anaconda prompt):\n",
    "\n",
    "`conda install pytorch-cpu torchvision-cpu -c pytorch`\n",
    "\n",
    "After this has successfully run, import the packages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Intuitive Intro to Neural Nets\n",
    "\n",
    "In today's lab, we're going to start with an intuitive exercise on the Titanic dataset using Logistic Regression and a simple Neural Network before moving onto some more complex stuff. Let's start by loading our dataset. \n",
    "\n",
    "Remember, the Titanic data is stored in a CSV file (located in the 'data' directory of your root folder), so we need to use Pandas to load the data and then separate it into our X (features) and y (target). We also need to: i) drop unimportant columns, and ii) impute missing values.\n",
    "\n",
    "We're going to do this all in the next cell - refer to the decision tree lab from yesterday for the detailed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  Sex   Age  SibSp  Parch     Fare\n",
       "PassengerId                                                    \n",
       "1                   0       3    1  22.0      1      0   7.2500\n",
       "2                   1       1    0  38.0      1      0  71.2833\n",
       "3                   1       3    0  26.0      0      0   7.9250\n",
       "4                   1       1    0  35.0      1      0  53.1000\n",
       "5                   0       3    1  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = pd.read_csv('data/titanic.csv', index_col = 0) # load the data\n",
    "data = data.drop(['Name', 'Ticket', 'Cabin', 'Embarked'], axis=1) # remove unimportant columns\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['Sex'])\n",
    "data['Sex'] = le.transform(data['Sex']) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as we have become accustomed to doing, we will split the dataset into a training set (where we will do our cross validation) and a test set (our hold-out data). We've done this a few times during the labs, so hopefully you're getting used to the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_data = data[\"Survived\"]\n",
    "feature_data = data.iloc[:, data.columns != \"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to try out some models on our training data (you haven't seen anything new yet!). Since we're solving a binary classification problem (i.e., predicting a 0 or 1 target), we want to design classifiers. So far in the course we've covered the following simple classifiers: **k-nearest neighbors**, **decision trees**, and **logistic regression**.\n",
    "\n",
    "In this exercise, we're going to fit a logistic regression to our data and then design a neural network architecture that behaves exactly like a logistic regression and validate that we get the same result.\n",
    "\n",
    "##### Recap: Logistic Regression\n",
    "\n",
    "Logistic regression models are linear models similar to linear regression models. Hopefully you somewhat remember them from lecture. Let's review them, starting with the linear regression equation:\n",
    "\n",
    "\n",
    "<center>$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n$</center>\n",
    "\n",
    "Where $\\hat{y}$ is our prediction, $\\beta$ is our vector of coefficients (the things we learn), and $x$ is our feature vector. The linear regression equation defines a line in $n$ dimensional space. The problem with linear regression is that it doesn't really perform well on classification tasks. Consider the following example:\n",
    "\n",
    "<img src=\"img/linear-classification.png\" width=\"500\"/>\n",
    "\n",
    "The green line represents our trained linear regression model. Our feature is the size of a tumor, and our target is whether it is malignant or not (0 or 1). As we can see, even though our model is trained to the data to minimize error, for a lot of the values of tumor size it is going to give us a weird result (e.g., for some really small tumors, the prediction would be a negative value!).\n",
    "\n",
    "To resolve this, we use the *logistic function* (also called the *sigmoid* function) to 'squish' our linear model to be bounded by 0 and 1. The logistic (sigmoid) function is $\\frac{1}{1+e^{-x}}$, and thus our logistic regression equation becomes:\n",
    "\n",
    "<center>$\\large\\hat{y} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n)}}$</center>\n",
    "\n",
    "In this equation, the values for $\\hat{y}$ can never be below 0, and can never exceed 1 even for the most extreme feature values $x$.\n",
    "\n",
    "**YOUR TURN:** \n",
    "* Assuming you've trained a nice logistic regression model to the below data (see Figure), what might the model fit look like (i.e., what will the line look like)? ____________________________________\n",
    "* For new data samples with features $x$, how would you convert the output of the logistic regression, $\\hat{y}$, into a classification (0 or 1)? ______________________________\n",
    "\n",
    "<img src=\"img/logistic-classification.jpg\" width=\"400\"/>\n",
    "\n",
    "OK, cool! So a quick review of logistic regression. Let's use scikit-learn to fit a logistic regression model to our training set and then predict on our test set (we won't do cross validation this time). Remember, when we used decision trees and tree ensembles, our cross validation accuracy was somewhere from 75-80%. \n",
    "\n",
    "*Note: Remember to first impute missing values!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy: 79.48 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "imp = Imputer(missing_values=np.nan, strategy='most_frequent') # Remember to impute missing values! (training set)\n",
    "imp.fit(X_train)\n",
    "\n",
    "X_train = imp.transform(X_train) \n",
    "\n",
    "logreg = LogisticRegression() \n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "X_test = imp.transform(X_test) # Remember to impute missing values! (test set)\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print (\"Logistic accuracy: %.2f\" % (accuracy * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so our logistic regression gives us similar performance to the tree methods we explored yesterday. \n",
    "\n",
    "**YOUR TURN:** \n",
    "* The default regularization parameter for sklearn's logistic regression is L2 (or ridge regression); can you figure out how to change it to L1 (LASSO)? ______________________\n",
    "* What is the mean accuracy of an L1 regularized Logistic regression model on the training set? ______________________\n",
    "\n",
    "OK - time for the good stuff!\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "A *neural network* (NN) is a type of machine learning model that, like linear or logistic regression, takes a feature vector, X, as input and predicts a target, y. The way it does this is a little bit different, however. A typical NN architecture consists of: an input layer, hidden layers, and an output layer. Each layer consists of a set of nodes (neurons) connected by edges (outputs). Let's look at the figure below:\n",
    "\n",
    "<img src=\"img/nn.jpeg\" width=\"400\"/>\n",
    "\n",
    "**Input layer**: This is a passive layer that simply takes in your feature data and outputs it to the hidden layers. You can think of each input layer neuron as being associated with a feature in your feature set.\n",
    "\n",
    "**Hidden layer**: This is where the magic happens. The original features, as received by the input layer, go through a series of transformations within the hidden layer. You can think of each node (neuron) within the hidden layer as a highly transformed feature. \n",
    "\n",
    "**Output layer**: This is where we get our final result, the 0 or 1 prediction.\n",
    "\n",
    "### Zooming In\n",
    "\n",
    "Let's take a look at what is happening at any given node (neuron) within the hidden layer. Take a look at the following image of a neuron within an NN:\n",
    "\n",
    "<img src=\"img/neuron.png\" width=\"500\"/>\n",
    "\n",
    "Every neuron has some inputs ($x_1, x_2, \\dots, x_n$) with input weights ($w_1, w_2, \\dots, w_n$) and an output, $Y$. The neuron itself applies a transformation, $f$, known as the *activation function*, to the linear combination of its inputs and input weights. The value $b$ is a constant weight called the bias.\n",
    "\n",
    "There are many different types of activation functions, but the popular ones are the *sigmoid*, *tanh*, and *ReLU* activation functions. Yes, you heard correctly: the sigmoid function is a popular activation function! (This should be reminding you of the logistic regression model we discussed above).\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If you were to develop a simple neural network architecture that was equivalent to a logistic regression model for the Titanic data, how would you do it? Get a pen and paper and draw it out. Make sure to specify: the input layer, the hidden layer(s), the output layer, the activation function(s), the weights, and the biases.\n",
    "* How many hidden layers does your NN have? What type of activation function, $f$, does it use? _________________\n",
    "* Say you wanted to add another layer to your NN architecture with 3x neurons, what would your new architecture look like? _________________\n",
    "\n",
    "### Intro to  PyTorch\n",
    "\n",
    "OK, so now that we've made the connection between NNs and Logistic Regression, let's code up our little NN in PyTorch and use it to predict survivorship on the Titanic dataset.\n",
    "\n",
    "First, *tensors* are the fundamental data type of PyTorch. Each tensor is effectively a multi-dimensional array, just like a numpy array. The primary difference is that tensors have been setup in such a way to enhance the NN training process.\n",
    "\n",
    "Let's load our X and y training data into tensors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "X_train_tensor = Variable(torch.Tensor(X_train))\n",
    "y_train_tensor = Variable(torch.Tensor(y_train.values))\n",
    "\n",
    "X_test_tensor = Variable(torch.Tensor(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will actually define our logistic regression network model class. The below function, `LogisticRegression`, applies a sigmoid transformation to the output, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we identify the dimensions of our problem: 6 x 2 (6 features and 2 target classes: 0 or 1), initialize our model with those dimensions and then specify the loss function ([cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)) and optimization technique ([stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 6\n",
    "output_dim = 2\n",
    "    \n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up our dataset to be *iterable* such that we can train our neural network in *batches*. A batch is a subset of the total data such that if we combined them all, we'd get the whole dataset. Batching is done to speed up the training process and reduce memory requirements.\n",
    "\n",
    "**YOUR TURN:** \n",
    "* If we select a batch size of 32, how many batches of training data will be generated?________________\n",
    "\n",
    "The `DataLoader` function does this batching operation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=X_train_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=X_test_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our NN and test its performance on the test set every 500 iterations. We use 3000 *epochs* and print our accuracy values every 4000 iterations. An epoch is when the entire dataset has passed through the network. An iteration is when a single forward/backward pass of the network over a batch of data is done. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* If our batch size is 32 and we elect to do 3000 epochs (i.e., the network sees the entire dataset 3000 times), how many iterations (forward/backward passes of the data) will our network see? _______________________\n",
    "\n",
    "Run the below code and check out the incremental accuracy improvement output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic NN test set accuracy: 36.57 after 4000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 8000 iterations\n",
      "Logistic NN test set accuracy: 35.45 after 12000 iterations\n",
      "Logistic NN test set accuracy: 35.45 after 16000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 20000 iterations\n",
      "Logistic NN test set accuracy: 35.45 after 24000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 28000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 32000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 36000 iterations\n",
      "Logistic NN test set accuracy: 35.45 after 40000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 44000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 48000 iterations\n",
      "Logistic NN test set accuracy: 35.82 after 52000 iterations\n",
      "Logistic NN test set accuracy: 35.45 after 56000 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6465419f0c3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "iterations = 0\n",
    "epochs = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for features, target in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "        if iterations % 4000 == 0:\n",
    "            outputs = model(X_test_tensor.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, np.array(predicted))\n",
    "            print (\"Logistic NN test set accuracy: %.2f after %d iterations\" % (accuracy * 100, iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you got an accuracy of around ~79%. What you'll notice is that is the same accuracy we got from sklearn's built-in logistic regression function from earlier in the lab! \n",
    "\n",
    "Let's take a look at the trained model parameters using the `model.parameters()` function within PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0519,  4.9908, -0.0232,  0.5389,  0.1785, -0.0101],\n",
       "         [-0.0523, -3.2392, -0.0981, -1.6221, -0.7108,  0.1835]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([-1.0191,  1.3480], requires_grad=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, well this is interesting! We can see that our model consists of two tensors: the first has (2,6), and the second has dimension (1,2). Refer back to how you drew what you thought this NN architecture would look like.\n",
    "\n",
    "**YOU TURN:**\n",
    "* What do you think these values represent? ____________________________\n",
    "* How many hidden layers does the architecture have? ______________________________\n",
    "* Draw the architecture and label (some of) the weights (trained parameters). ______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this took considerably longer to train than scikit-learn's logistic regression: this is because PyTorch is set-up to be more flexible and train architectures much more complex than a simple single neuron network. Scikit-learn's implementation of logistic regression is highly optimized. \n",
    "\n",
    "**YOUR TURN:**\n",
    "* How many epochs would you need to increase the process to 100,000 iterations? ______________________\n",
    "* Does increasing to 100,000 iterations improve your test set accuracy? ______________________\n",
    "* Compare the predictions of your logistic regression from scikit-learn and your network developed with PyTorch. Are all the predictions the same? How many predictions are pair-wise different? ______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've completed an introduction to neural networks and PyTorch. If you want to explore more sophisicated architectures and applications, check out designing a PyTorch neural network to properly classify digit images here:\n",
    "\n",
    "https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
    "\n",
    "Other than that, you're done the lab!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

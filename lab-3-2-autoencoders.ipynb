{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UofT FASE ML Bootcamp\n",
    "#### Wednesday, August 21\n",
    "#### Autoencoders - Lab 3, Day 2 \n",
    "#### Teaching team: Elias Khalil, Kyle E. C. Booth, and Alex Olson \n",
    "##### Lab author: Alexander Olson, aolson@mie.utoronto.ca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yesterday we looked at a few methods of dimensionality reduction - PCA and t-SNE. Today we are going to learn about a _deep_ model for dimensionality reduction: Autoencoders.\n",
    "\n",
    "An autoencoder is a special type of neural network with an unusual task: for some input X, all it has to do is return that input X as accurately as possible. But there's a catch, of course! Between the input and the output, the number of nodes in each hidden layer actually gets progressively _smaller_. This means that in the first half of the network, the network must learn how to represent the input in ever more compact formats. The second half does this in reverse, taking the smallest representation of the input and expanding it back out into the full, original data.\n",
    "\n",
    "<img src=\"img/ae.png\" alt=\"cross-val\" width=\"500\"/>\n",
    "\n",
    "Autoencoders are much more powerful than PCA and t-SNE when it comes to learning compact representations of data, as we will see here. Let's first bring back PCA and give it an autoencoder's task: first, reduce the dimensionality of a dataset, and then recover the full dimensionality of the original input.\n",
    "\n",
    "We will use MNIST again today, although through a slightly different mechanism to ease its compatibility with Pytorch. Run the code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path parameters\n",
    "MNIST_PATH = Path('./mnist/')\n",
    "DOWNLOAD_MNIST = not MNIST_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training data size:\\t',train_data.data.size())     # (60000, 28, 28)\n",
    "print('Testing data size:\\t',train_data.targets.size())   # (60000)\n",
    "plt.imshow(train_data.data[2].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.targets[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TURN**\n",
    "\n",
    "Using PCA, reduce the dimensionality of the MNIST dataset to two dimensions. Then, using the `inverse_transform` method of the PCA object, transform the data back to its original size and plot the same data point shown above.\n",
    "\n",
    "Note that to use the input data with the PCA library, you'll need to use Numpy's `reshape` command to convert a 60000x28x28 vector to 60000x784. Then on the other end, you'll need to convert back to 60000x28x28 in order to plot. For help with the `reshape` command, check [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How does the image look after dimensionality reduction compared to the input? ______\n",
    "* Why might it look this way? ______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on to building an autoencoder for the same task. Autoencoders are easy networks to build, split into the _encoder_, which 'steps' the data down to the final compact representation, and the _decoder_, which is a mirror image of the encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myEncoder = nn.Sequential(\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 12),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(12, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may expect, designing the structure of the encoder is something of an art, and it requires balance between the time and input data required to train the network, and performance. Here we are using four step-down operations (the linear layers), which take us from the input of size 784 down to just two dimensions at the bottom. Between each step-down layer is a non-linear activation layer.\n",
    "\n",
    "**Your turn**\n",
    "\n",
    "It would of course be possible to go straight from the input size to the final number of dimensions, but we would lose an incredibly important aspect of neural networks in doing so.\n",
    "\n",
    "* What would we miss out on? ______\n",
    "* Why is this a problem? ______\n",
    "\n",
    "In the cell below, build the structure of the decoder layer for our network. Remember, this is a mirror image of our encoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDecoder = nn.Sequential(\n",
    "    ###Your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just need some boilerplate class code to bring the whole thing together into a PyTorch network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = myEncoder\n",
    "        self.decoder = myDecoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an instance of our network. We also need to define a few other parameters, like the loss function and the optimizer.\n",
    "\n",
    "For the loss function, we will be using Mean Squared Error, which we covered in the second lab. For the optimizer, let's use an advanced optimizer called Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.005)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a helper function during training which will pass us the data as we go. It's important to remember that for an autoencoder, the input and the label are identical, so we don't have any labels per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training function is going to show us the recovered images at the end of each epoch, to help us get an idea of how the training process is going. Beforehand, we will just plot out five of the digits so we can compare our autoencoder's output to what the target looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST_IMG = 5\n",
    "# initialize figure\n",
    "f, a = plt.subplots(1, N_TEST_IMG, figsize=(5, 2))\n",
    "# original data (first row) for viewing\n",
    "view_data = train_data.data[:N_TEST_IMG].view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "for i in range(N_TEST_IMG):\n",
    "    a[i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap='gray'); a[i].set_xticks(()); a[i].set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we're ready to train! Before running this code, make sure you understand what is happening between the dashed lines (the rest is just there to draw the graphs, so it's not important). The comments should help. Once you feel comfortable, run the cell! It will take a little while to complete, but you will get progress updates as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    for step, (x, _) in enumerate(train_loader):\n",
    "        #----------------------------------------------------------------------------#\n",
    "        b_x = x.view(-1, 28*28)   # batch x, shape (batch, 28*28)\n",
    "        b_y = x.view(-1, 28*28)   # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = loss_func(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "        #----------------------------------------------------------------------------#\n",
    "    print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())\n",
    "    f, a = plt.subplots(1, N_TEST_IMG, figsize=(5, 2))\n",
    "\n",
    "    # plotting decoded image (second row)\n",
    "    _, decoded_data = autoencoder(view_data)\n",
    "    for i in range(N_TEST_IMG):\n",
    "        a[i].clear()\n",
    "        a[i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap='gray')\n",
    "        a[i].set_xticks(()); a[i].set_yticks(())\n",
    "    plt.draw(); plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third image in the column is our Autoencoder's recovered version of the number 4 we looked at with PCA. How does it look? Could it be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the encoded data in the same way we did for PCA and t-SNE in the last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "view_data = train_data.data.view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "encoded_data, _ = autoencoder(view_data)\n",
    "X, Y = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy()\n",
    "values = train_data.targets.numpy()\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(X, Y,\n",
    "            c=values, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "Finally, briefly, we will look at VAEs. This is a special type of autoencoder in which the hidden representation takes the form of a statistical distribution - specifically, a Gaussian or Normal distribution. The power of this representation is that only two values - the mean and standard deviation - need to be learned in order to represent a Gaussian. It's highly compact! The downside, of course, is that this only works if the data _can_ be represented this way, but in practice VAEs work really well for a wide variety of datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch,train_loader, view_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
